import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.distributions.empirical_distribution import ECDF
from hnn_nuts_olm import nuts_hnn_sample, get_model
from get_args import get_args
from pathlib import Path
from datetime import datetime
import time
import logging
from utils import setup_logger, compute_average_ess


def setup_logger(name):
    """
    Set up a logger for tracking the experiment.

    This function creates a log file with a timestamped name in the 'logs' directory
    and configures a logger to log messages to both the console and the file.

    Parameters
    ----------
    name : str
        The name of the logger.

    Returns
    -------
    logging.Logger
        Configured logger instance.
    """
    # ** Create the 'logs' folder if it doesn't exist **
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)

    # ** Create a timestamped log file name **
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = log_dir / f'{name}_{timestamp}.log'

    # ** Configure the logger **
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)

    # File handler for logging to a file
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)

    # Console handler for logging to the console
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)

    # Set log message format
    formatter = logging.Formatter('%(asctime)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)

    logger.addHandler(fh)
    logger.addHandler(ch)

    return logger


def plot_figure11(samples_nuts, samples_lhnn, burn_in):
    """
    Generate Figure 11: Scatter plot matrix for the Allen-Cahn equation.

    This function visualizes the sampling results from NUTS and LHNN-NUTS methods
    for the Allen-Cahn equation as a scatter plot matrix.

    Parameters
    ----------
    samples_nuts : np.ndarray
        Samples generated by NUTS.
    samples_lhnn : np.ndarray
        Samples generated by LHNN-NUTS.
    burn_in : int
        Number of initial samples to discard as burn-in.
    """
    logger = logging.getLogger('Allen_Cahn_Comparison')
    logger.info("Creating scatter plot matrix...")

    # ** Remove burn-in samples **
    samples_nuts = samples_nuts[:, burn_in:, :]
    samples_lhnn = samples_lhnn[:, burn_in:, :]

    # ** Select dimensions to plot (from the paper) **
    dims_to_plot = [0, 11, 12, 24]  # Corresponds to dimensions 1, 12, 13, 25
    n_dims = len(dims_to_plot)

    # ** Create the figure and subplots **
    fig, axes = plt.subplots(n_dims, n_dims, figsize=(15, 15))

    # Set colors and transparency
    nuts_color = 'blue'
    lhnn_color = 'red'
    alpha = 0.5

    # ** Iterate over all combinations of dimensions **
    for i in range(n_dims):
        for j in range(n_dims):
            ax = axes[i, j]
            dim_i = dims_to_plot[i]
            dim_j = dims_to_plot[j]

            if i == j:
                # ** Diagonal: Kernel Density Estimation (KDE) **
                nuts_data = samples_nuts[0, :, dim_i]
                lhnn_data = samples_lhnn[0, :, dim_i]

                # Compute x-axis range
                min_val = min(np.min(nuts_data), np.min(lhnn_data))
                max_val = max(np.max(nuts_data), np.max(lhnn_data))
                x = np.linspace(min_val, max_val, 200)

                # KDE computation (replace `plt.mlab.GaussianKDE` with `scipy.stats.gaussian_kde`)
                from scipy.stats import gaussian_kde
                nuts_kde = gaussian_kde(nuts_data)
                lhnn_kde = gaussian_kde(lhnn_data)

                # Plot KDE curves
                ax.plot(x, nuts_kde(x), color=nuts_color, alpha=0.8, label='NUTS')
                ax.plot(x, lhnn_kde(x), color=lhnn_color, alpha=0.8, label='LHNN-NUTS')
            else:
                # ** Off-diagonal: Scatter plot **
                ax.scatter(samples_nuts[0, :, dim_j], samples_nuts[0, :, dim_i],
                           alpha=alpha, color=nuts_color, s=1, label='NUTS')
                ax.scatter(samples_lhnn[0, :, dim_j], samples_lhnn[0, :, dim_i],
                           alpha=alpha, color=lhnn_color, s=1, label='LHNN-NUTS')

            # ** Set axis labels **
            if i == n_dims - 1:  # Bottom row
                ax.set_xlabel(f'u{dims_to_plot[j] + 1}')
            if j == 0:  # Leftmost column
                ax.set_ylabel(f'u{dims_to_plot[i] + 1}')

            # Add grid lines
            ax.grid(True, linestyle='--', alpha=0.3)

    # ** Add legend (only in the first subplot) **
    axes[0, 0].legend()
    plt.tight_layout()

    # ** Save the figure **
    figures_dir = Path("figures")
    figures_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = figures_dir / f'figure_11_reproduction_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    logger.info(f"Plot saved to {filename}")


def compute_allen_cahn_metrics(samples_nuts, nuts_grads,
                               samples_lhnn, lhnn_monitoring_grads,
                               burn_in=1000):
    """
    Compute performance metrics for the Allen-Cahn equation.

    This function computes:
    1. Average Effective Sample Size (ESS) for NUTS and LHNN-NUTS.
    2. Total number of gradients evaluated during sampling.
    3. ESS per gradient for both methods.

    Parameters
    ----------
    samples_nuts : np.ndarray
        Samples generated by NUTS.
    nuts_grads : int
        Total number of gradients evaluated by NUTS.
    samples_lhnn : np.ndarray
        Samples generated by LHNN-NUTS.
    lhnn_monitoring_grads : int
        Total number of gradients evaluated during error monitoring by LHNN-NUTS.
    burn_in : int, optional
        Number of initial samples to discard as burn-in. Defaults to 1000.

    Returns
    -------
    dict
        A dictionary containing average ESS, total gradients, and ESS per gradient
        for NUTS and LHNN-NUTS.
    """
    logger = logging.getLogger('Allen_Cahn_Comparison')
    logger.info("Computing metrics...")

    # ** Compute average ESS for NUTS and LHNN-NUTS **
    avg_ess_nuts = compute_average_ess(samples_nuts, burn_in)
    avg_ess_lhnn = compute_average_ess(samples_lhnn, burn_in)

    # ** Compute training gradients for LHNN-NUTS **
    training_grads = 25 * 10 * 40  # Mt=25, T=10, 40 steps per unit

    # ** Compute total gradient evaluations **
    total_grads_nuts = nuts_grads
    total_grads_lhnn = training_grads + lhnn_monitoring_grads

    # ** Compute ESS per gradient **
    ess_per_grad_nuts = avg_ess_nuts / total_grads_nuts
    ess_per_grad_lhnn = avg_ess_lhnn / total_grads_lhnn

    # ** Log performance metrics **
    logger.info("\n=== Allen-Cahn Performance Comparison ===")
    logger.info(f"NUTS - Average ESS: {avg_ess_nuts:.2f}")
    logger.info(f"NUTS - Total gradients: {total_grads_nuts}")
    logger.info(f"NUTS - ESS per gradient: {ess_per_grad_nuts:.6f}")

    logger.info(f"\nLHNN-NUTS - Average ESS: {avg_ess_lhnn:.2f}")
    logger.info(f"LHNN-NUTS - Evaluation gradients: {lhnn_monitoring_grads}")
    logger.info(f"LHNN-NUTS - Training gradients: {training_grads}")
    logger.info(f"LHNN-NUTS - Total gradients: {total_grads_lhnn}")
    logger.info(f"LHNN-NUTS - ESS per gradient: {ess_per_grad_lhnn:.6f}")

    return {
        'NUTS': {'ess': avg_ess_nuts, 'grads': total_grads_nuts,
                 'ess_per_grad': ess_per_grad_nuts},
        'LHNN': {'ess': avg_ess_lhnn, 'grads': total_grads_lhnn,
                 'ess_per_grad': ess_per_grad_lhnn}
    }


def run_allen_cahn_comparison():
    """
    Run the comparison experiments for the Allen-Cahn equation.

    This function:
    1. Configures the sampling parameters for NUTS and LHNN-NUTS.
    2. Runs the sampling for both methods.
    3. Computes performance metrics (ESS, gradients, ESS per gradient).
    4. Generates Figure 11 to visualize the comparison.
    """
    logger = setup_logger('Allen_Cahn_Comparison')
    start_time = time.time()

    # ** Set up parameters **
    args = get_args()
    args.dist_name = 'Allen_Cahn'
    args.input_dim = 50  # 25D Allen-Cahn (position + momentum = 50 dimensions)
    args.latent_dim = 25
    args.total_samples = 20  # Total samples
    args.burn_in = 0  # Burn-in samples
    args.nuts_step_size = 0.025
    args.hnn_error_threshold = 10.0
    args.num_chains = 1
    args.n_cooldown = 20

    logger.info("Starting Allen-Cahn comparison with parameters:")
    for arg in vars(args):
        logger.info(f"{arg}: {getattr(args, arg)}")

    # ** Run traditional NUTS **
    logger.info("\n===== Running Traditional NUTS =====")
    model_trad = get_model(args)
    samples_nuts, accept_nuts, errors_nuts, nuts_grads = nuts_hnn_sample(
        model_trad, args, traditional_only=True
    )
    logger.info(f"NUTS sampling completed. Accept rate: {np.mean(accept_nuts.numpy()):.4f}")

    # ** Run LHNN-NUTS **
    logger.info("\n===== Running LHNN-NUTS =====")
    model_lhnn = get_model(args)
    model_lhnn.load_weights(f"{args.save_dir}/Allen_Cahn10")
    samples_lhnn, accept_lhnn, errors_lhnn, lhnn_monitoring_grads = nuts_hnn_sample(
        model_lhnn, args, traditional_only=False
    )
    logger.info(f"LHNN-NUTS sampling completed. Accept rate: {np.mean(accept_lhnn.numpy()):.4f}")

    # ** Generate Figure 11 **
    plot_figure11(samples_nuts, samples_lhnn, burn_in=args.burn_in)

    # ** Compute and log metrics **
    metrics = compute_allen_cahn_metrics(
        samples_nuts, nuts_grads,
        samples_lhnn, lhnn_monitoring_grads,
        burn_in=args.burn_in
    )

    elapsed_time = time.time() - start_time
    logger.info(f"Total execution time: {elapsed_time:.2f} seconds")

    return metrics


if __name__ == "__main__":
    """
    Main script to run the comparison experiments for the Allen-Cahn equation.
    """
    import os

    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    run_allen_cahn_comparison()